{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Vanilla Neural Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image classifer with logistic regression\n",
    "#images shape(width,height,3(rgb))\n",
    "\n",
    "import numpy as np\n",
    "import scipy \n",
    "import os\n",
    "import matplotlib.pyplot as plot\n",
    "import math\n",
    "from scipy import ndimage\n",
    "\n",
    "#from basicFunctions import crossEntropyLoss \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropyLoss(a,Y):\n",
    "    num_of_examples = Y.shape[1] #len(Y) doesn't work, need 2nd dimesnion\n",
    "    a = a.T\n",
    "    loss = - (1 / num_of_examples) * np.sum(np.dot((1 - Y), np.log(1 - a)) + np.dot(Y, np.log(a)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing cross entropy function\n",
    "test_y = np.asarray([[1,1,1]])\n",
    "test_x = np.asarray([[.8,.9,.4]])\n",
    "print(crossEntropyLoss(test_x,test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output = [1231. 12312. 1231. ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    output = 1.0/(1.0+np.exp(-x))\n",
    "    cached_value = x\n",
    "    cached_value = {\"out_a\":x}\n",
    "    return output, cached_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x,leaky=0):\n",
    "    output = np.maximum(leaky,x)\n",
    "#     cached_value = x\n",
    "    cached_value = {\"out_a\":x}\n",
    "    return output, cached_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To generate numbers randomly with numpy, we will use np.random.randn(dimension_x, dimension_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params_shallow(input_layer, hidden_layer, output_layer):\n",
    "    w1 = np.random.randn(hidden_layer,input_layer)\n",
    "    w2 = np.random.randn(output_layer,hidden_layer)\n",
    "    #np.random.randn gives numbers larger than 1 so: (vanishing/exploading gradients)\n",
    "    w1,w2 = w1 * .01, w2 * .01\n",
    "    \n",
    "    #is it by one because of broadcasting \n",
    "    b1 = np.zeros((hidden_layer,1))\n",
    "    b2 = np.zeros((output_layer,1))\n",
    "    \n",
    "    assert(w1.shape == (hidden_layer, input_layer))\n",
    "    assert(w2.shape == (output_layer, hidden_layer))\n",
    "    assert(b1.shape == (hidden_layer,1))\n",
    "    assert(b2.shape == (output_layer,1))\n",
    "    \n",
    "    return {\"w1\":w1,\"w2\":w2,\"b1\":b1,\"b2\":b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_shallow = initialize_params_shallow(3, 4, 2)\n",
    "print(\"w1:\", params_shallow[\"w1\"])\n",
    "print(\"w2:\", params_shallow[\"w2\"])\n",
    "print(\"b1:\", params_shallow[\"b1\"])\n",
    "print(\"b2:\", params_shallow[\"b2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params_deep(input_layer, hidden_layers, output_layer):\n",
    "#     np.random.seed(3) #non random random numbers (will get the same random numbers each time) used for testing\n",
    "    layers = {} #dictionary of weights and biasis\n",
    "    previous_layer = input_layer #sets the begining shape as shape of input layer\n",
    "    for i in range(len(hidden_layers)):\n",
    "        cur_layer = hidden_layers[i]\n",
    "        weights = np.random.randn(cur_layer,previous_layer)\n",
    "        weights *= .01 #because np.random.rand returns numbers too large, more efficeint durring gradient descensed \n",
    "        bias = np.zeros((cur_layer,1))\n",
    "        cur_weights_key = \"w\" + str(i + 1) # i + 1 because humans like things indexed at 0\n",
    "        cur_bias_key = \"b\" + str(i + 1)\n",
    "        layers[cur_weights_key] = weights\n",
    "        layers[cur_bias_key] = bias \n",
    "        \n",
    "        assert(layers[\"w\" + str(i + 1)].shape == (cur_layer, previous_layer))\n",
    "        assert(layers[\"b\" + str(i + 1)].shape == (cur_layer,1))\n",
    "        \n",
    "        previous_layer = cur_layer\n",
    "\n",
    "    layers[\"w\" + str(len(hidden_layers) + 1)] = np.random.randn(output_layer,previous_layer) * .01\n",
    "    layers[\"b\" + str(len(hidden_layers) + 1)] = np.zeros((output_layer,1))\n",
    "\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 (6, 5)\n",
      "b1 (6, 1)\n",
      "w2 (7, 6)\n",
      "b2 (7, 1)\n",
      "w3 (10, 7)\n",
      "b3 (10, 1)\n",
      "w4 (3, 10)\n",
      "b4 (3, 1)\n"
     ]
    }
   ],
   "source": [
    "params_deep1 = initialize_params_deep(5,[4],3)\n",
    "for param in params_deep2:\n",
    "    print(param, params_deep2[param].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_deep2 = initialize_params_deep(5,[6,7,10],3)\n",
    "for param in params_deep2:\n",
    "    print(param, params_deep2[param].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward_prop(a,w,b): #a because a is from the past layer\n",
    "    w = w.T\n",
    "    a = a.T\n",
    "    z = np.dot(a,w) + b\n",
    "#     print(\"z\",z.shape,\"a\",a.shape[0],\"w\",w.shape[1],\"b\",b.shape)\n",
    "    assert(z.shape == (a.shape[0], w.shape[1] ))\n",
    "    #     assert(z.shape == np.dot(a,w).shape)\n",
    "    cached_inputs = {\"in_a\":a,\"w\":w,\"b\":b} #(a,w,b)\n",
    "    return z,cached_inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (3, 2) w (1, 3) b (1,) z (2, 1)\n",
      "{'a': array([[ 1.62434536, -0.52817175,  0.86540763],\n",
      "       [-0.61175641, -1.07296862, -2.3015387 ]]), 'w': array([[ 1.74481176],\n",
      "       [-0.7612069 ],\n",
      "       [ 0.3190391 ]]), 'b': array([-0.24937038])}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.randn(3,2)\n",
    "w = np.random.randn(1,3)\n",
    "b = np.random.randn(1) #(1,1) broadcasting\n",
    "\n",
    "X = np.array(X)\n",
    "w = np.array(w)\n",
    "b = np.array(b)\n",
    "z,_ = linear_forward_prop(X,w,b)\n",
    "print(\"X\",X.shape,\"w\",w.shape,\"b\",b.shape,\"z\",z.shape)\n",
    "print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singleFowardPropagation(X,w,b,activationType,leaky=0): #TODO change x to a\n",
    "#     cached_inputs = {\"w\":None,\"x\":None,\"b\":None,\"z\":None}\n",
    "    z,cached_values = linear_forward_prop(X,w,b)\n",
    "    if activationType == \"sigmoid\":\n",
    "        a, activation_cache = sigmoid(z)\n",
    "    if activationType == \"relu\":\n",
    "        a, activation_cache = relu(z,leaky=leaky)\n",
    "#     final_cache = c\n",
    "    cached_values.update(activation_cache)\n",
    "    return a, cached_values  #add asserts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A [[ 3.43896131]\n",
      " [ 0.        ]]\n",
      "cached_values {'in_a': array([[-0.41675785, -2.1361961 , -1.79343559],\n",
      "       [-0.05626683,  1.64027081, -0.84174737]]), 'w': array([[ 0.50288142],\n",
      "       [-1.24528809],\n",
      "       [-1.05795222]]), 'b': array([[-0.90900761]]), 'out_a': array([[ 3.43896131],\n",
      "       [-2.08938436]])}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "X = np.random.randn(3,2)\n",
    "w = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1) #(1,1) broadcasting\n",
    "a,cached_values = singleFowardPropagation(X,w,b,\"relu\")\n",
    "print(\"A\",a)\n",
    "print(\"cached_values\",cached_values)\n",
    "# print(X,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fowardPropagate(X,params):\n",
    "    num_layers = len(params)/2\n",
    "    a = X\n",
    "    cached_values = []\n",
    "    for i in range(1,num_layers):\n",
    "        w = params[\"w\" + str(i)]\n",
    "        b = params[\"b\" + str(i)]\n",
    "        past_a = a\n",
    "        a, cur_cached = singleFowardProgagation(past_a,w,b,\"relu\")\n",
    "        cached_values += [cur_cached] #TODO: get foramt of cached values \n",
    "    w = params[\"w\" + len(num_layers)]\n",
    "    b = params[\"b\" + len(num_layers)]\n",
    "    a, cur_cached = singleFowardProgagation(a,w,b,\"sigmoid\")\n",
    "    cached_values += [cur_cached] \n",
    "    #write assert statemt here\n",
    "    #get shape of a at the end \n",
    "    \n",
    "    return a, cached_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
