{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Vanilla Neural Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Image classifer with logistic regression\n",
    "#images shape(width,height,3(rgb))\n",
    "\n",
    "###to knit to html ipython nbconvert --to html [NEWFILENAME].html\n",
    "import numpy as np\n",
    "import scipy \n",
    "import os\n",
    "import matplotlib.pyplot as plot\n",
    "import math\n",
    "from scipy import ndimage\n",
    "\n",
    "#from basicFunctions import crossEntropyLoss \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Constants\n",
    "side_length = 100\n",
    "image_path = \"./Logistic_Regression_Data/\"\n",
    "test_ratio = .3\n",
    "epoch = 7000\n",
    "learning_rate = .2\n",
    "max_num_images = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getImageArrays(path, side_length): #returns list of images arrays for a specified path\n",
    "    image_names = os.listdir(path)\n",
    "    image_names = image_names[:max_num_images]\n",
    "    examples = []\n",
    "    for image_name in image_names:\n",
    "        if image_name.split(\".\")[-1] != \"DS_Store\":\n",
    "            try:\n",
    "                cur_image_path = path + image_name\n",
    "                cur_image = np.array(ndimage.imread(cur_image_path,flatten=False))\n",
    "                cur_array_resized = scipy.misc.imresize(cur_image,size=(side_length,side_length))\n",
    "                cur_array_flattened = cur_array_resized.reshape((side_length*side_length*3)).T\n",
    "\n",
    "                examples += [cur_array_flattened] \n",
    "            except ValueError:\n",
    "                print(\"Error in creating examples\",image_name)\n",
    "                \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in creating examples notcow126.jpg\n",
      "Number of Examples: 597\n",
      "Number of Labels: 597\n"
     ]
    }
   ],
   "source": [
    "#create examples & labels\n",
    "cow_images_path = image_path + \"cows/\"\n",
    "notCow_image_path = image_path + \"notcows/\"\n",
    "\n",
    "examples_cow = getImageArrays(cow_images_path, side_length)\n",
    "labels_cow = np.ones(len(examples_cow))\n",
    "examples_notCow = getImageArrays(notCow_image_path, side_length)\n",
    "labels_notCow = np.zeros(len(examples_notCow))\n",
    "\n",
    "examples_cow = np.array(examples_cow)\n",
    "examples_notCow = np.array(examples_notCow)\n",
    "#assert(examples_cow.shape[1:] == (side_length,side_length,3)), \"examples_cow are invalid shape\"\n",
    "examples = np.concatenate((examples_cow,examples_notCow))\n",
    "labels = np.concatenate((labels_cow,labels_notCow))\n",
    "print(\"Number of Examples:\",len(examples))\n",
    "print(\"Number of Labels:\",len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffled_indexing = np.random.permutation(labels.shape[0])\n",
    "examples = examples[shuffled_indexing]\n",
    "labels = labels[shuffled_indexing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:  (30000, 418)\n",
      "Number of test examples:  (30000, 179)\n",
      "(418,)\n"
     ]
    }
   ],
   "source": [
    "#seperate train and test examples\n",
    "number_examples_test = int(len(examples)*test_ratio)\n",
    "number_labels_test = int(len(labels)*test_ratio)\n",
    "\n",
    "examples_test = examples[:number_examples_test]\n",
    "examples_train = examples[number_examples_test:]\n",
    "labels_test = labels[:number_labels_test]\n",
    "labels_train = labels[number_labels_test:]\n",
    "print(\"Number of training examples: \", examples_train.T.shape)\n",
    "print(\"Number of test examples: \", examples_test.T.shape)\n",
    "print(labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reshape labels for future matrix operations\n",
    "labels_train = np.reshape(labels_train,(1,len(labels_train)))\n",
    "labels_test = np.reshape(labels_test,(1,len(labels_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#flatten examples\n",
    "# flattened_train_examples = examples_train.reshape(examples_train.shape[0], -1).T\n",
    "# flattened_test_examples = examples_test.reshape(examples_test.shape[0], -1).T  \n",
    "# print(\"flattened examples\",flattened_test_examples.shape,flattened_train_examples.shape)\n",
    "flattened_train_examples = examples_train.T\n",
    "flattened_test_examples = examples_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized (30000, 179) (30000, 418)\n"
     ]
    }
   ],
   "source": [
    "# Standardize color values of the image (decrease computational cost durring cross entropy)\n",
    "standardized_train_examples = flattened_train_examples/255 #225 is the maximum rgb value/ This is done to decrease varaince in inputs thus more efficint\n",
    "standardized_test_examples = flattened_test_examples/255\n",
    "print(\"standardized\",standardized_test_examples.shape,standardized_train_examples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossEntropyLoss(a,Y):\n",
    "    num_of_examples = Y.shape[1] #len(Y) doesn't work, need 2nd dimesnion\n",
    "    a = a.T\n",
    "    loss = - (1.0 / num_of_examples) * (np.dot((1.0 - Y), np.log(1.0 - a)) + np.dot(Y, np.log(a)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    output = 1.0/(1.0+np.exp(-x))\n",
    "    cached_value = x\n",
    "    cached_value = {\"out_a\":x}\n",
    "    return output, cached_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x,leaky=0):\n",
    "    output = np.maximum(leaky,x)\n",
    "#     cached_value = x\n",
    "    cached_value = {\"out_a\":x}\n",
    "    return output, cached_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params_shallow(input_layer, hidden_layer, output_layer):\n",
    "    w1 = np.random.randn(hidden_layer,input_layer)\n",
    "    w2 = np.random.randn(output_layer,hidden_layer)\n",
    "    #np.random.randn gives numbers larger than 1 so: (vanishing/exploading gradients)\n",
    "    w1,w2 = w1 * .01, w2 * .01\n",
    "    \n",
    "    #is it by one because of broadcasting \n",
    "    b1 = np.zeros((hidden_layer,1))\n",
    "    b2 = np.zeros((output_layer,1))\n",
    "    \n",
    "    assert(w1.shape == (hidden_layer, input_layer))\n",
    "    assert(w2.shape == (output_layer, hidden_layer))\n",
    "    assert(b1.shape == (hidden_layer,1))\n",
    "    assert(b2.shape == (output_layer,1))\n",
    "    \n",
    "    return {\"w1\":w1,\"w2\":w2,\"b1\":b1,\"b2\":b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params_deep(input_layer, hidden_layers, output_layer):\n",
    "#     np.random.seed(3) #non random random numbers (will get the same random numbers each time) used for testing\n",
    "    layers = {} #dictionary of weights and biasis\n",
    "    previous_layer = input_layer #sets the begining shape as shape of input layer\n",
    "    for i in range(len(hidden_layers)):\n",
    "        cur_layer = hidden_layers[i]\n",
    "        weights = np.random.randn(cur_layer,previous_layer)\n",
    "        weights *= .01 #because np.random.rand returns numbers too large, more efficeint durring gradient descensed \n",
    "        bias = np.zeros((cur_layer,1))\n",
    "        cur_weights_key = \"w\" + str(i + 1) # i + 1 because humans like things indexed at 0\n",
    "        cur_bias_key = \"b\" + str(i + 1)\n",
    "        layers[cur_weights_key] = weights\n",
    "        layers[cur_bias_key] = bias \n",
    "        \n",
    "        assert(layers[\"w\" + str(i + 1)].shape == (cur_layer, previous_layer))\n",
    "        assert(layers[\"b\" + str(i + 1)].shape == (cur_layer,1))\n",
    "        \n",
    "        previous_layer = cur_layer\n",
    "\n",
    "    layers[\"w\" + str(len(hidden_layers) + 1)] = np.random.randn(output_layer,previous_layer) * .01\n",
    "    layers[\"b\" + str(len(hidden_layers) + 1)] = np.zeros((output_layer,1))\n",
    "\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward_prop(a,w,b): #a because a is from the past layer\n",
    "#     print(\"Before Transpose -- A:\",a.shape,\"W:\",w.shape,\"B:\",b.shape)\n",
    "#     print(\"After Transpose -- A:\",a.shape,\"W:\",w.shape,\"B:\",b.shape)\n",
    "    z = np.dot(w,a) + b\n",
    "#     print(\"z:\",z.shape)\n",
    "#     print(\"z\",z.shape,\"a\",a.shape[0],\"w\",w.shape[1],\"b\",b.shape)\n",
    "#     assert(z.shape == (a.shape[0], w.shape[1] ))\n",
    "    #     assert(z.shape == np.dot(a,w).shape)\n",
    "    cached_inputs = {\"in_a\":a,\"w\":w,\"b\":b,\"z\":z} #(a,w,b)\n",
    "    return z,cached_inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singleFowardPropagation(X,w,b,activationType,leaky=0): #TODO change x to a\n",
    "#     cached_inputs = {\"w\":None,\"x\":None,\"b\":None,\"z\":None}\n",
    "    z,cached_values = linear_forward_prop(X,w,b)\n",
    "    if activationType == \"sigmoid\":\n",
    "        a, activation_cache = sigmoid(z)\n",
    "    if activationType == \"relu\":\n",
    "        a, activation_cache = relu(z,leaky=leaky)\n",
    "    cached_values.update(activation_cache)\n",
    "    return a, cached_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fowardPropagate(X,params):\n",
    "    num_layers = len(params) // 2 #// rounds down\n",
    "    #assert len params is even\n",
    "    a = X\n",
    "    cached_values = []\n",
    "    for i in range(1,num_layers):\n",
    "        w = params[\"w\" + str(i)]\n",
    "        b = params[\"b\" + str(i)]\n",
    "        past_a = a\n",
    "        a, cur_cached = singleFowardPropagation(past_a,w,b,\"relu\")\n",
    "        cached_values += [cur_cached] #TODO: get foramt of cached values \n",
    "        \n",
    "    w = params[\"w\" + str(num_layers)]\n",
    "    b = params[\"b\" + str(num_layers)]\n",
    "    a, cur_cached = singleFowardPropagation(a,w,b,\"sigmoid\")\n",
    "    cached_values += [cur_cached] \n",
    "    #write assert statemt here\n",
    "    \n",
    "    return a, cached_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearBackprop(cached_values, dZ):\n",
    "    a = cached_values[\"in_a\"]\n",
    "    w = cached_values[\"w\"]\n",
    "    b = cached_values[\"b\"]\n",
    "    number_examples = a.shape[1]\n",
    "    dW = (1.0/number_examples) * np.dot(dZ,a.T)\n",
    "    dB = (1.0/number_examples) * np.sum(dZ, axis = 1, keepdims= True)\n",
    "    dA = np.dot(w.T, dZ)\n",
    "    return {\"dW\":dW,\"dB\":dB,\"dA\":dA}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, z):\n",
    "    dZ = np.array(dA, copy=True) #convert dA to np array\n",
    "    dZ[z <= 0] = 0\n",
    "    return dZ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, z):\n",
    "    s = 1/(1 + np.exp(-z))\n",
    "    dz = dA * s * (1-s) \n",
    "    \n",
    "    return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backwards_activation(dA, cached_values, activation_type):\n",
    "    if activation_type == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, cached_values[\"z\"])\n",
    "    if activation_type == \"relu\":\n",
    "        dZ = relu_backward(dA, cached_values[\"z\"])\n",
    "    gradients = linearBackprop(cached_values, dZ)\n",
    "    return gradients\n",
    "    \n",
    "    #todo write optimizer function\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backprop(a, cached_values, y):\n",
    "    gradients = {}\n",
    "    num_layers = len(cached_values)\n",
    "    y = y.reshape(a.shape)\n",
    "\n",
    "    dFinalA = -(np.divide(y,a) - np.divide(1-y, 1-a)) #dervivative of output of foward pass \n",
    "    backwards_activation_output_0 = backwards_activation(dFinalA, cached_values[-1],\"sigmoid\")\n",
    "    \n",
    "    gradients[\"dW\" + str(num_layers)] = backwards_activation_output_0[\"dW\"]\n",
    "    gradients[\"dB\" + str(num_layers)] = backwards_activation_output_0[\"dB\"]\n",
    "    gradients[\"dA\" + str(num_layers - 1)] = backwards_activation_output_0[\"dA\"]\n",
    "    \n",
    "    for i in reversed(range(num_layers - 1)):\n",
    "        past_layer = cached_values[i]\n",
    "        cur_dLayer = backwards_activation(gradients[\"dA\" + str(i + 1)], past_layer, \"relu\")\n",
    "        gradients[\"dW\" + str(i + 1)] = cur_dLayer[\"dW\"]\n",
    "        gradients[\"dB\" + str(i + 1)] = cur_dLayer[\"dB\"]\n",
    "        gradients[\"dA\" + str(i)] = cur_dLayer[\"dA\"]\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(grads,params, learning_rate):\n",
    "    num_layers = len(params)//2\n",
    "    for i in range(num_layers): #subtraction\n",
    "#         print(\"lr\",learning_rate)\n",
    "#         print(\"w-before (\", str(i+1), \"): \", params[\"w\" + str(i +1)])\n",
    "#         print(\"b-before (\", str(i+1), \"): \", params[\"b\" + str(i +1)])\n",
    "#         print(\"------------\")\n",
    "#         print(\"dW\",grads[\"dW\" + str(i + 1)])\n",
    "#         print(\"dB\",grads[\"dB\" + str(i + 1)])\n",
    "        params[\"w\" + str(i +1)] = params[\"w\" + str(i + 1)] - (grads[\"dW\" + str(i + 1)] * learning_rate)\n",
    "        params[\"b\" + str(i + 1)] = params[\"b\" + str(i + 1)] - (grads[\"dB\" + str(i + 1)] * learning_rate)\n",
    "#         print(\"w-after (\", str(i+1), \"): \", params[\"w\" + str(i +1)])\n",
    "#         print(\"b-after (\", str(i+1), \"): \", params[\"b\" + str(i +1)])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA0 (1, 2)\n",
      "dW1 (3, 4)\n",
      "dB1 (3, 1)\n",
      "dW2 (1, 3)\n",
      "dB2 (1, 1)\n",
      "dA1 (3, 2)\n",
      "{'dW2': array([[-0.39202432, -0.13325855, -0.04601089]]), 'dB2': array([[ 0.15187861]]), 'dA1': array([[-0.14175655,  0.48317296],\n",
      "       [ 0.01663708, -0.05670698],\n",
      "       [ 0.20472941, -0.69781407]]), 'dW1': array([[-0.45019464, -0.08570497, -0.15147489, -0.11528942],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.65018572,  0.1237779 ,  0.21876495,  0.16650473]]), 'dB1': array([[ 0.24158648],\n",
      "       [ 0.        ],\n",
      "       [-0.34890704]]), 'dA0': array([[ 0.        ,  1.43247838]])}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "a = np.random.randn(1, 2)\n",
    "a1 = np.random.randn(4, 2)\n",
    "b1 = np.random.randn(3, 4)\n",
    "w1 = np.random.randn(3, 1)\n",
    "z1 = np.random.randn(3, 2)\n",
    "cache1 = [{\"in_a\" : a1,\"b\" : b1,\"w\" : w1, \"z\" : z1}]\n",
    "a2 = np.random.randn(3, 2)\n",
    "b2 = np.random.randn(1, 1)\n",
    "w2 = np.random.randn(1, 3)\n",
    "z2 = np.random.randn(1, 2)\n",
    "cache2 = [{\"in_a\" : a2,\"b\" : b2,\"w\" : w2, \"z\" : z2}]\n",
    "megaCache = cache1 +  cache2\n",
    "y = np.array([[1,0]])\n",
    "# print(\"y\",y)\n",
    "# print(megaCache[0][\"in_a\"],\"in_a1\")\n",
    "# print(megaCache[0][\"b\"],\"b1\")\n",
    "# print(megaCache[0][\"w\"],\"w1\")\n",
    "# print(megaCache[0][\"z\"],\"z1\")\n",
    "# print(megaCache[1][\"in_a\"],\"in_a2\")\n",
    "# print(megaCache[1][\"b\"],\"b2\")\n",
    "# print(megaCache[1][\"w\"],\"w2\")\n",
    "# print(megaCache[1][\"z\"],\"z2\")\n",
    "\n",
    "\n",
    "grads = backprop(a, megaCache,y)\n",
    "print(\"dA0\", grads[\"dA0\"].shape)\n",
    "print(\"dW1\", grads[\"dW1\"].shape)\n",
    "print(\"dB1\", grads[\"dB1\"].shape)\n",
    "print(\"dW2\", grads[\"dW2\"].shape)\n",
    "print(\"dB2\", grads[\"dB2\"].shape)\n",
    "print(\"dA1\", grads[\"dA1\"].shape)\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizer(params,examples,learing_rate,epoch):\n",
    "    for i in range(epoch):\n",
    "        for X,y in examples:\n",
    "            a, cache = fowardPropagate(X,params)\n",
    "            grads = backprop(a,params,cache,Y)\n",
    "            for key,value in params:\n",
    "                if key[1] != \"A\": \n",
    "                    cur_grad = grads[\"d\" + key]\n",
    "                    value = value - (learning_rate * grad)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = 12288 #change to side length*length*3 \n",
    "hidden_layer = 7\n",
    "output_layer = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shallowNN(X,y,epoch,lr,input_layer,hidden_layer,output_layer):\n",
    "    params = initialize_params_shallow(input_layer,hidden_layer,output_layer)\n",
    "    costs = []\n",
    "    grads = {}\n",
    "    for i in range(epoch):\n",
    "        w1 = params[\"w1\"]\n",
    "        w2 = params[\"w2\"]\n",
    "        b1 = params[\"b1\"]\n",
    "        b2 = params[\"b2\"]\n",
    "        a1, cached_values1 = singleFowardPropagation(X,w1,b1,\"relu\")\n",
    "        a2, cached_values2 = singleFowardPropagation(a1,w2,b2,\"sigmoid\")\n",
    "        # backwards [end] (1,710) --> (7,710) --> (exmample size,710) [start]\n",
    "        cost = crossEntropyLoss(a2,y)\n",
    "        dFinalA = -(np.divide(y,a2) - np.divide(1.0-y, 1.0-a2))\n",
    "#         print(\"dfinal a\", dFinalA.shape)\n",
    "        backwards_activation_output_0 = backwards_activation(dFinalA, cached_values2,\"sigmoid\")\n",
    "#         print(\"penultimate activation\",np.shape(backwards_activation_output_0[\"dA\"]))\n",
    "        backwards_activation_output_1 = backwards_activation(backwards_activation_output_0[\"dA\"], cached_values1,\"relu\")\n",
    "#         print(\"last activation\",np.shape(backwards_activation_output_1[\"dA\"]))\n",
    "        grads[\"dW1\"] = backwards_activation_output_1[\"dW\"]\n",
    "        grads[\"dW2\"] = backwards_activation_output_0[\"dW\"]\n",
    "        grads[\"dB1\"] = backwards_activation_output_1[\"dB\"]\n",
    "        grads[\"dB2\"] = backwards_activation_output_0[\"dB\"]\n",
    "        params = optimize(grads, params, learning_rate)\n",
    "        \n",
    "        costs += [cost]\n",
    "        if i % 100 == 0:\n",
    "            print(\"Costs @ \" + str(i) + \":\",cost)    \n",
    "    return params, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shallowNNTest(X,y,params):\n",
    "    w1 = params[\"w1\"]\n",
    "    w2 = params[\"w2\"]\n",
    "    b1 = params[\"b1\"]\n",
    "    b2 = params[\"b2\"]\n",
    "    a1, cached_values1 = singleFowardPropagation(X,w1,b1,\"relu\")\n",
    "    a2, cached_values2 = singleFowardPropagation(a1,w2,b2,\"sigmoid\")\n",
    "    loss = np.mean(np.abs(a2-y))\n",
    "    print(\"Test Accuracey:\",loss * 100,\"%\")\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Costs @ 0: [[ 0.6929272]]\n",
      "Costs @ 100: [[ 0.66869672]]\n",
      "Costs @ 200: [[ 0.53534395]]\n",
      "Costs @ 300: [[ 0.50558393]]\n",
      "Costs @ 400: [[ 0.46910628]]\n",
      "Costs @ 500: [[ 0.43264576]]\n",
      "Costs @ 600: [[ 0.4073861]]\n",
      "Costs @ 700: [[ 0.38078119]]\n",
      "Costs @ 800: [[ 0.33458718]]\n",
      "Costs @ 900: [[ 0.29633872]]\n",
      "Costs @ 1000: [[ 0.27114652]]\n",
      "Costs @ 1100: [[ 0.60099022]]\n",
      "Costs @ 1200: [[ 0.14425561]]\n",
      "Costs @ 1300: [[ 0.12016897]]\n",
      "Costs @ 1400: [[ 0.10470029]]\n",
      "Costs @ 1500: [[ 0.14135025]]\n",
      "Costs @ 1600: [[ 0.06584272]]\n",
      "Costs @ 1700: [[ 0.05406976]]\n",
      "Costs @ 1800: [[ 0.04516935]]\n",
      "Costs @ 1900: [[ 0.03740821]]\n"
     ]
    }
   ],
   "source": [
    "X = standardized_train_examples\n",
    "y = labels_train\n",
    "input_layer = side_length * side_length * 3\n",
    "hidden_layer = 7\n",
    "output_layer = 1\n",
    "learning_rate = .01\n",
    "params,costs = shallowNN(X,y,2000,learning_rate,input_layer,hidden_layer,output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracey: 23.3877261869 %\n"
     ]
    }
   ],
   "source": [
    "X = standardized_test_examples\n",
    "y = labels_test\n",
    "loss = shallowNNTest(X,y,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
