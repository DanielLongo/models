{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Vanilla Neural Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Image classifer with logistic regression\n",
    "#images shape(width,height,3(rgb))\n",
    "\n",
    "###to knit to html ipython nbconvert --to html [NEWFILENAME].html\n",
    "import numpy as np\n",
    "import scipy \n",
    "import os\n",
    "import matplotlib.pyplot as plot\n",
    "import math\n",
    "from scipy import ndimage\n",
    "\n",
    "#from basicFunctions import crossEntropyLoss \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Constants\n",
    "side_length = 64\n",
    "image_path = \"./Logistic_Regression_Data/\"\n",
    "test_ratio = .3\n",
    "epoch = 7000\n",
    "learning_rate = .005\n",
    "max_num_images = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getImageArrays(path, side_length): #returns list of images arrays for a specified path\n",
    "    image_names = os.listdir(path)\n",
    "    image_names = image_names[:max_num_images]\n",
    "    examples = []\n",
    "    for image_name in image_names:\n",
    "        if image_name.split(\".\")[-1] != \"DS_Store\":\n",
    "            try:\n",
    "                cur_image_path = path + image_name\n",
    "                cur_image = np.array(ndimage.imread(cur_image_path,flatten=False))\n",
    "                cur_array_resized = scipy.misc.imresize(cur_image,size=(side_length,side_length))\n",
    "\n",
    "    #             img = Image.fromarray(cur_array_resized, 'RGB')\n",
    "    #             img.save('my.png')\n",
    "    #             img.show()\n",
    "                cur_array_flattened = cur_array_resized.reshape((side_length*side_length*3)).T\n",
    "                #print(\"new\", cur_array_resized.shape)\n",
    "                #cur_array = scipy.ndimage.imread(cur_image_path,flatten=False) #reads image as numpy array (lenght,height,3)\n",
    "                #cur_array_resized = scipy.misc.imresize(cur_array,size=(side_length,side_length,3)) #resizes images to a uniform shape\n",
    "                #print(\"old\", cur_array_resized.shape)\n",
    "                examples += [cur_array_flattened] \n",
    "            except ValueError:\n",
    "                print(\"Error in creating examples\",image_name)\n",
    "                \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in creating examples notcow197.jpg\n",
      "Error in creating examples notcow20.jpg\n",
      "Error in creating examples notcow38.jpg\n",
      "Error in creating examples notcow415.jpg\n",
      "Number of Examples: 994\n",
      "Number of Labels: 994\n"
     ]
    }
   ],
   "source": [
    "#create examples & labels\n",
    "cow_images_path = image_path + \"cows/\"\n",
    "notCow_image_path = image_path + \"notcows/\"\n",
    "\n",
    "examples_cow = getImageArrays(cow_images_path, side_length)\n",
    "labels_cow = np.ones(len(examples_cow))\n",
    "examples_notCow = getImageArrays(notCow_image_path, side_length)\n",
    "labels_notCow = np.zeros(len(examples_notCow))\n",
    "\n",
    "examples_cow = np.array(examples_cow)\n",
    "examples_notCow = np.array(examples_notCow)\n",
    "#assert(examples_cow.shape[1:] == (side_length,side_length,3)), \"examples_cow are invalid shape\"\n",
    "examples = np.concatenate((examples_cow,examples_notCow))\n",
    "labels = np.concatenate((labels_cow,labels_notCow))\n",
    "print(\"Number of Examples:\",len(examples))\n",
    "print(\"Number of Labels:\",len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffled_indexing = np.random.permutation(labels.shape[0])\n",
    "examples = examples[shuffled_indexing]\n",
    "labels = labels[shuffled_indexing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:  (12288, 696)\n",
      "Number of test examples:  (12288, 298)\n",
      "(696,)\n"
     ]
    }
   ],
   "source": [
    "#seperate train and test examples\n",
    "number_examples_test = int(len(examples)*test_ratio)\n",
    "number_labels_test = int(len(labels)*test_ratio)\n",
    "\n",
    "examples_test = examples[:number_examples_test]\n",
    "examples_train = examples[number_examples_test:]\n",
    "labels_test = labels[:number_labels_test]\n",
    "labels_train = labels[number_labels_test:]\n",
    "print(\"Number of training examples: \", examples_train.T.shape)\n",
    "print(\"Number of test examples: \", examples_test.T.shape)\n",
    "print(labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reshape labels for future matrix operations\n",
    "labels_train = np.reshape(labels_train,(1,len(labels_train)))\n",
    "labels_test = np.reshape(labels_test,(1,len(labels_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#flatten examples\n",
    "# flattened_train_examples = examples_train.reshape(examples_train.shape[0], -1).T\n",
    "# flattened_test_examples = examples_test.reshape(examples_test.shape[0], -1).T  \n",
    "# print(\"flattened examples\",flattened_test_examples.shape,flattened_train_examples.shape)\n",
    "flattened_train_examples = examples_train.T\n",
    "flattened_test_examples = examples_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardized (12288, 298) (12288, 696)\n"
     ]
    }
   ],
   "source": [
    "# Standardize color values of the image (decrease computational cost durring cross entropy)\n",
    "standardized_train_examples = flattened_train_examples/255 #225 is the maximum rgb value/ This is done to decrease varaince in inputs thus more efficint\n",
    "standardized_test_examples = flattened_test_examples/255\n",
    "print(\"standardized\",standardized_test_examples.shape,standardized_train_examples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossEntropyLoss(a,Y):\n",
    "    num_of_examples = Y.shape[1] #len(Y) doesn't work, need 2nd dimesnion\n",
    "    a = a.T\n",
    "    loss = - (1.0 / num_of_examples) * (np.dot((1.0 - Y), np.log(1.0 - a)) + np.dot(Y, np.log(a)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.4149316]]\n"
     ]
    }
   ],
   "source": [
    "# Testing cross entropy function\n",
    "test_y = np.asarray([[1,1,1]])\n",
    "test_x = np.asarray([[.8,.9,.4]])\n",
    "print(crossEntropyLoss(test_x,test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    output = 1.0/(1.0+np.exp(-x))\n",
    "    cached_value = x\n",
    "    cached_value = {\"out_a\":x}\n",
    "    return output, cached_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x,leaky=0):\n",
    "    output = np.maximum(leaky,x)\n",
    "#     cached_value = x\n",
    "    cached_value = {\"out_a\":x}\n",
    "    return output, cached_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To generate numbers randomly with numpy, we will use np.random.randn(dimension_x, dimension_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params_shallow(input_layer, hidden_layer, output_layer):\n",
    "    w1 = np.random.randn(hidden_layer,input_layer)\n",
    "    w2 = np.random.randn(output_layer,hidden_layer)\n",
    "    #np.random.randn gives numbers larger than 1 so: (vanishing/exploading gradients)\n",
    "    w1,w2 = w1 * .01, w2 * .01\n",
    "    \n",
    "    #is it by one because of broadcasting \n",
    "    b1 = np.zeros((hidden_layer,1))\n",
    "    b2 = np.zeros((output_layer,1))\n",
    "    \n",
    "    assert(w1.shape == (hidden_layer, input_layer))\n",
    "    assert(w2.shape == (output_layer, hidden_layer))\n",
    "    assert(b1.shape == (hidden_layer,1))\n",
    "    assert(b2.shape == (output_layer,1))\n",
    "    \n",
    "    return {\"w1\":w1,\"w2\":w2,\"b1\":b1,\"b2\":b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1: [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]\n",
      " [ 0.01744812 -0.00761207  0.00319039]\n",
      " [-0.0024937   0.01462108 -0.02060141]]\n",
      "w2: [[-0.00322417 -0.00384054  0.01133769 -0.01099891]\n",
      " [-0.00172428 -0.00877858  0.00042214  0.00582815]]\n",
      "b1: [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "b2: [[ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "params_shallow = initialize_params_shallow(3, 4, 2)\n",
    "print(\"w1:\", params_shallow[\"w1\"])\n",
    "print(\"w2:\", params_shallow[\"w2\"])\n",
    "print(\"b1:\", params_shallow[\"b1\"])\n",
    "print(\"b2:\", params_shallow[\"b2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test case for initialize_params_shallow using seed(1)\n",
    "##### (3,4,2)\n",
    "<table style:\"width-80%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "         <b> w1:</b> [[ 0.01624345 -0.00611756 -0.00528172]\n",
    "         [-0.01072969  0.00865408 -0.02301539]\n",
    "         [ 0.01744812 -0.00761207  0.00319039]\n",
    "         [-0.0024937   0.01462108 -0.02060141]]\n",
    "        </td>\n",
    "        <td>\n",
    "        w2: [[-0.00322417 -0.00384054  0.01133769 -0.01099891]\n",
    " [-0.00172428 -0.00877858  0.00042214  0.00582815]]\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "        b1: [[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]\n",
    "        </td>\n",
    "        <td>\n",
    "b2: [[ 0.]\n",
    " [ 0.]]\n",
    " </td>\n",
    "     </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params_deep(input_layer, hidden_layers, output_layer):\n",
    "#     np.random.seed(3) #non random random numbers (will get the same random numbers each time) used for testing\n",
    "    layers = {} #dictionary of weights and biasis\n",
    "    previous_layer = input_layer #sets the begining shape as shape of input layer\n",
    "    for i in range(len(hidden_layers)):\n",
    "        cur_layer = hidden_layers[i]\n",
    "        weights = np.random.randn(cur_layer,previous_layer)\n",
    "        weights *= .01 #because np.random.rand returns numbers too large, more efficeint durring gradient descensed \n",
    "        bias = np.zeros((cur_layer,1))\n",
    "        cur_weights_key = \"w\" + str(i + 1) # i + 1 because humans like things indexed at 0\n",
    "        cur_bias_key = \"b\" + str(i + 1)\n",
    "        layers[cur_weights_key] = weights\n",
    "        layers[cur_bias_key] = bias \n",
    "        \n",
    "        assert(layers[\"w\" + str(i + 1)].shape == (cur_layer, previous_layer))\n",
    "        assert(layers[\"b\" + str(i + 1)].shape == (cur_layer,1))\n",
    "        \n",
    "        previous_layer = cur_layer\n",
    "\n",
    "    layers[\"w\" + str(len(hidden_layers) + 1)] = np.random.randn(output_layer,previous_layer) * .01\n",
    "    layers[\"b\" + str(len(hidden_layers) + 1)] = np.zeros((output_layer,1))\n",
    "\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w1': array([[-0.01743141, -0.0059665 , -0.00588594, -0.00873882,  0.00029714],\n",
      "       [-0.02248258, -0.00267762,  0.01013183,  0.00852798,  0.01108187],\n",
      "       [ 0.01119391,  0.01487543, -0.01118301,  0.00845833, -0.0186089 ],\n",
      "       [-0.00602885, -0.01914472,  0.01048148,  0.01333738, -0.00197415]]), 'b1': array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]]), 'w2': array([[ 0.01774645, -0.00674728,  0.00150617,  0.00152946],\n",
      "       [-0.01064195,  0.00437947,  0.01938978, -0.01024931],\n",
      "       [ 0.00899338, -0.00154507,  0.01769627,  0.00483788]]), 'b2': array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]])}\n",
      "w1 (4, 5)\n",
      "b1 (4, 1)\n",
      "w2 (3, 4)\n",
      "b2 (3, 1)\n"
     ]
    }
   ],
   "source": [
    "params_deep1 = initialize_params_deep(5,[4],3)\n",
    "print(params_deep1)\n",
    "for param in params_deep1:\n",
    "    print(param, params_deep1[param].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 (4, 5)\n",
      "[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 (4, 1)\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "w2 (3, 4)\n",
      "[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 (3, 1)\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "params_deep2 = initialize_params_deep(5,[4],3)\n",
    "for param in params_deep2:\n",
    "    print(param, params_deep2[param].shape)\n",
    "    print(params_deep2[param])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testcase initialize_params_deep \n",
    "##### seed(3) (5,[3],3)\n",
    "<table style:\"width-80%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "w1 (4, 5)\n",
    "[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
    " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
    " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
    " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
    " </td>\n",
    " <td>\n",
    "b1 (4, 1)\n",
    "[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]\n",
    "</td>\n",
    "<tr>\n",
    "<td>\n",
    "w2 (3, 4)\n",
    "[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
    " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
    " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
    "</td>\n",
    "<td>\n",
    "b2 (3, 1)\n",
    "[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]\n",
    " </td>\n",
    " </tr>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward_prop(a,w,b): #a because a is from the past layer\n",
    "    print(\"Before Transpose -- A:\",a.shape,\"W:\",w.shape,\"B:\",b.shape)\n",
    "    print(\"After Transpose -- A:\",a.shape,\"W:\",w.shape,\"B:\",b.shape)\n",
    "    z = np.dot(w,a) + b\n",
    "    print(\"z:\",z.shape)\n",
    "#     print(\"z\",z.shape,\"a\",a.shape[0],\"w\",w.shape[1],\"b\",b.shape)\n",
    "#     assert(z.shape == (a.shape[0], w.shape[1] ))\n",
    "    #     assert(z.shape == np.dot(a,w).shape)\n",
    "    cached_inputs = {\"in_a\":a,\"w\":w,\"b\":b} #(a,w,b)\n",
    "    return z,cached_inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X [[ 1.62434536 -0.61175641]\n",
      " [-0.52817175 -1.07296862]\n",
      " [ 0.86540763 -2.3015387 ]] w [[ 1.74481176 -0.7612069   0.3190391 ]] b [-0.24937038] z [[ 3.26295337 -1.23429987]]\n",
      "Before Transpose -- A: (3, 2) W: (1, 3) B: (1,)\n",
      "After Transpose -- A: (3, 2) W: (1, 3) B: (1,)\n",
      "z: (1, 2)\n",
      "X (3, 2) w (1, 3) b (1,) z (1, 2)\n",
      "[[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.randn(3,2)\n",
    "w = np.random.randn(1,3)\n",
    "b = np.random.randn(1) #(1,1) broadcasting\n",
    "print(\"X\",X,\"w\",w,\"b\",b,\"z\",z)\n",
    "z,_ = linear_forward_prop(X,w,b)\n",
    "print(\"X\",X.shape,\"w\",w.shape,\"b\",b.shape,\"z\",z.shape)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test case for linear_foward_prop\n",
    "##### seed(1) x=(3,2) w=(1,3) b=(1)\n",
    "<table style:\"width-80%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "X (3, 2) w (1, 3) b (1,) z (2, 1)\n",
    "</td>\n",
    "</tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "[[ 3.26295337]\n",
    " [-1.23429987]]\n",
    " </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singleFowardPropagation(X,w,b,activationType,leaky=0): #TODO change x to a\n",
    "#     cached_inputs = {\"w\":None,\"x\":None,\"b\":None,\"z\":None}\n",
    "    z,cached_values = linear_forward_prop(X,w,b)\n",
    "    if activationType == \"sigmoid\":\n",
    "        a, activation_cache = sigmoid(z)\n",
    "    if activationType == \"relu\":\n",
    "        a, activation_cache = relu(z,leaky=leaky)\n",
    "#     final_cache = c\n",
    "    cached_values.update(activation_cache)\n",
    "    return a, cached_values  #add asserts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Transpose -- A: (3, 2) W: (1, 3) B: (1, 1)\n",
      "After Transpose -- A: (2, 3) W: (3, 1) B: (1, 1)\n",
      "z: (2, 1)\n",
      "Relu A [[ 3.43896131]\n",
      " [ 0.        ]]\n",
      "Before Transpose -- A: (3, 2) W: (1, 3) B: (1, 1)\n",
      "After Transpose -- A: (2, 3) W: (3, 1) B: (1, 1)\n",
      "z: (2, 1)\n",
      "Sigmoud A [[ 0.96890023]\n",
      " [ 0.11013289]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "X = np.random.randn(3,2)\n",
    "w = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1) #(1,1) broadcasting\n",
    "a,cached_values = singleFowardPropagation(X,w,b,\"relu\")\n",
    "print(\"Relu A\",a)\n",
    "\n",
    "a,cached_values = singleFowardPropagation(X,w,b,\"sigmoid\")\n",
    "print(\"Sigmoud A\",a)\n",
    "# print(X,w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu A [[ 3.43896131]\n",
    " [ 0.        ]]\n",
    "Sigmoud A [[ 0.96890023]\n",
    " [ 0.11013289]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fowardPropagate(X,params):\n",
    "    print(\"len of params\", len(params))\n",
    "    num_layers = len(params) // 2 #// rounds down\n",
    "    #assert len params is even\n",
    "    a = X\n",
    "    cached_values = []\n",
    "    for i in range(1,num_layers):\n",
    "        w = params[\"w\" + str(i)]\n",
    "        b = params[\"b\" + str(i)]\n",
    "        past_a = a\n",
    "        a, cur_cached = singleFowardPropagation(past_a,w,b,\"relu\")\n",
    "        cached_values += [cur_cached] #TODO: get foramt of cached values \n",
    "        \n",
    "    w = params[\"w\" + str(num_layers)]\n",
    "    b = params[\"b\" + str(num_layers)]\n",
    "    a, cur_cached = singleFowardPropagation(a,w,b,\"sigmoid\")\n",
    "    cached_values += [cur_cached] \n",
    "    #write assert statemt here\n",
    "    \n",
    "    return a, cached_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of params 6\n",
      "Before Transpose -- A: (5, 4) W: (4, 5) B: (4, 1)\n",
      "After Transpose -- A: (5, 4) W: (4, 5) B: (4, 1)\n",
      "z: (4, 4)\n",
      "Before Transpose -- A: (4, 4) W: (3, 4) B: (3, 1)\n",
      "After Transpose -- A: (4, 4) W: (3, 4) B: (3, 1)\n",
      "z: (3, 4)\n",
      "Before Transpose -- A: (3, 4) W: (1, 3) B: (1, 1)\n",
      "After Transpose -- A: (3, 4) W: (1, 3) B: (1, 1)\n",
      "z: (1, 4)\n",
      "a: [[ 0.03921668  0.70498921  0.19734387  0.04728177]]\n",
      "cached_values [{'in_a': array([[-0.31178367,  0.72900392,  0.21782079, -0.8990918 ],\n",
      "       [-2.48678065,  0.91325152,  1.12706373, -1.51409323],\n",
      "       [ 1.63929108, -0.4298936 ,  2.63128056,  0.60182225],\n",
      "       [-0.33588161,  1.23773784,  0.11112817,  0.12915125],\n",
      "       [ 0.07612761, -0.15512816,  0.63422534,  0.810655  ]]), 'w': array([[ 0.35480861,  1.81259031, -1.3564758 , -0.46363197,  0.82465384],\n",
      "       [-1.17643148,  1.56448966,  0.71270509, -0.1810066 ,  0.53419953],\n",
      "       [-0.58661296, -1.48185327,  0.85724762,  0.94309899,  0.11444143],\n",
      "       [-0.02195668, -2.12714455, -0.83440747, -0.46550831,  0.23371059]]), 'b': array([[ 1.38503523],\n",
      "       [-0.51962709],\n",
      "       [-0.78015214],\n",
      "       [ 0.95560959]]), 'out_a': array([[-5.23825714,  3.18040136,  0.4074501 , -1.88612721],\n",
      "       [-2.77358234, -0.56177316,  3.18141623, -0.99209432],\n",
      "       [ 4.18500916, -1.78006909, -0.14502619,  2.72141638],\n",
      "       [ 5.05850802, -1.25674082, -3.54566654,  3.82321852]])}, {'in_a': array([[ 0.        ,  3.18040136,  0.4074501 ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  3.18141623,  0.        ],\n",
      "       [ 4.18500916,  0.        ,  0.        ,  2.72141638],\n",
      "       [ 5.05850802,  0.        ,  0.        ,  3.82321852]]), 'w': array([[-0.12673638, -1.36861282,  1.21848065, -0.85750144],\n",
      "       [-0.56147088, -1.0335199 ,  0.35877096,  1.07368134],\n",
      "       [-0.37550472,  0.39636757, -0.47144628,  2.33660781]]), 'b': array([[ 1.50278553],\n",
      "       [-0.59545972],\n",
      "       [ 0.52834106]]), 'out_a': array([[  2.2644603 ,   1.09971298,  -2.90298027,   1.54036335],\n",
      "       [  6.33722569,  -2.38116246,  -4.11228806,   4.48582383],\n",
      "       [ 10.37508342,  -0.66591468,   1.63635185,   8.17870169]])}, {'in_a': array([[  2.2644603 ,   1.09971298,   0.        ,   1.54036335],\n",
      "       [  6.33722569,   0.        ,   0.        ,   4.48582383],\n",
      "       [ 10.37508342,   0.        ,   1.63635185,   8.17870169]]), 'w': array([[ 0.9398248 ,  0.42628539, -0.75815703]]), 'b': array([[-0.16236698]]), 'out_a': array([[-3.19864676,  0.87117055, -1.40297864, -3.00319435]])}]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6)\n",
    "X = np.random.randn(5,4)\n",
    "w1 = np.random.randn(4,5)\n",
    "b1 = np.random.randn(4,1)\n",
    "w2 = np.random.randn(3,4)\n",
    "b2 = np.random.randn(3,1)\n",
    "w3 = np.random.randn(1,3)\n",
    "b3 = np.random.randn(1,1)\n",
    "params = {\n",
    "    \"w1\":w1,\n",
    "    \"b1\":b1,\n",
    "    \"w2\":w2,\n",
    "    \"b2\":b2,\n",
    "    \"w3\":w3,\n",
    "    \"b3\":b3\n",
    "}\n",
    "a, cached_values = fowardPropagate(X, params)\n",
    "print(\"a:\",a)\n",
    "print(\"cached_values\",cached_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Transpose -- A: (5, 4) W: (4, 5) B: (4, 1)\n",
    "After Transpose -- A: (5, 4) W: (4, 5) B: (4, 1)\n",
    "z: (4, 4)\n",
    "Before Transpose -- A: (4, 4) W: (3, 4) B: (3, 1)\n",
    "After Transpose -- A: (4, 4) W: (3, 4) B: (3, 1)\n",
    "z: (3, 4)\n",
    "Before Transpose -- A: (3, 4) W: (1, 3) B: (1, 1)\n",
    "After Transpose -- A: (3, 4) W: (1, 3) B: (1, 1)\n",
    "z: (1, 4)\n",
    "a: [[ 0.03921668  0.70498921  0.19734387  0.04728177]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearBackprop(cached_values, dZ):\n",
    "    a = cached_values[\"in_a\"]\n",
    "    w = cached_values[\"w\"]\n",
    "    b = cached_values[\"b\"]\n",
    "    number_examples = a.shape[1]\n",
    "    dW = (1.0/number_examples) * np.dot(dZ,a.T)\n",
    "    dB = (1.0/number_examples) * np.sum(dZ, axis = 1, keepdims= True)\n",
    "    dA = np.dot(w.T, dZ)\n",
    "#write asserts for dW dB & dA TODO\n",
    "    return {\"dW\":dW,\"dB\":dB,\"dA\":dA}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dA': array([[ 0.51822968, -0.19517421],\n",
       "        [-0.40506361,  0.15255393],\n",
       "        [ 2.37496825, -0.89445391]]),\n",
       " 'dB': array([[ 0.50629448]]),\n",
       " 'dW': array([[-0.10076895,  1.40685096,  1.64992505]])}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "dZ = np.random.randn(1,2)\n",
    "a = np.random.randn(3,2)\n",
    "w = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "cached_values = {\"w\": w, \"in_a\":a,\"b\":b}\n",
    "linearBackprop(cached_values, dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'dA': array([[ 0.51822968, -0.19517421],\n",
    "        [-0.40506361,  0.15255393],\n",
    "        [ 2.37496825, -0.89445391]]),\n",
    " 'dB': array([[ 0.50629448]]),\n",
    " 'dW': array([[-0.10076895,  1.40685096,  1.64992505]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, z):\n",
    "    dZ = np.array(dA, copy=True) #convert dA to np array\n",
    "    dZ[z <= 0] = 0\n",
    "    return dZ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0  0]\n"
     ]
    }
   ],
   "source": [
    "dZ = relu_backward([-1,3,2], np.array([2,-1,0]))\n",
    "print(dZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, z):\n",
    "    s = 1/(1 + np.exp(-z))\n",
    "    dz = dA * s * (1-s) \n",
    "    \n",
    "    return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backwards_activation(dA, cached_values, activation_type):\n",
    "    if activation_type == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, cached_values[\"z\"])\n",
    "    if activation_type == \"relu\":\n",
    "        dZ = relu_backward(dA, cached_values[\"z\"])\n",
    "    gradients = linearBackprop(cached_values, dZ)\n",
    "    return gradients\n",
    "    \n",
    "    #todo write optimizer function\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINERAR BACKPROP\n",
      "w.T (3, 1)\n",
      "dZ (1, 2)\n",
      "LINERAR BACKPROP\n",
      "w.T (3, 1)\n",
      "dZ (1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dA': array([[ 0.11017994,  0.01105339],\n",
       "        [ 0.09466817,  0.00949723],\n",
       "        [-0.05743092, -0.00576154]]),\n",
       " 'dB': array([[-0.05729622]]),\n",
       " 'dW': array([[ 0.10266786,  0.09778551, -0.01968084]])}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_backwards_activation():\n",
    "# returns input parameters of backwards_activation\n",
    "np.random.seed(2)\n",
    "dA = np.random.randn(1,2)\n",
    "a = np.random.randn(3,2)\n",
    "w = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "z = np.random.randn(1,2)\n",
    "cached_values = {\"w\": w, \"in_a\":a,\"b\":b, \"z\":z}\n",
    "backwards_activation(dA, cached_values, activation_type=\"relu\")\n",
    "backwards_activation(dA, cached_values, activation_type=\"sigmoid\")\n",
    "#working\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Relu:\n",
    "{'dA': array([[ 0.44090989,  0.        ],\n",
    "        [ 0.37883606,  0.        ],\n",
    "        [-0.2298228 ,  0.        ]]),\n",
    " 'dB': array([[-0.20837892]]),\n",
    " 'dW': array([[ 0.44513824,  0.37371418, -0.10478989]])} \n",
    " \n",
    "For Sigmoid:\n",
    "{'dA': array([[ 0.11017994,  0.01105339],\n",
    "        [ 0.09466817,  0.00949723],\n",
    "        [-0.05743092, -0.00576154]]),\n",
    " 'dB': array([[-0.05729622]]),\n",
    " 'dW': array([[ 0.10266786,  0.09778551, -0.01968084]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backprop(a, cached_values, y):\n",
    "    gradients = {}\n",
    "    num_layers = len(cached_values)\n",
    "    y = y.reshape(a.shape)\n",
    "\n",
    "    dFinalA = -(np.divide(y,a) - np.divide(1-y, 1-a)) #dervivative of output of foward pass \n",
    "    backwards_activation_output_0 = backwards_activation(dFinalA, cached_values[-1],\"sigmoid\")\n",
    "    \n",
    "    gradients[\"dW\" + str(num_layers)] = backwards_activation_output_0[\"dW\"]\n",
    "    gradients[\"dB\" + str(num_layers)] = backwards_activation_output_0[\"dB\"]\n",
    "    gradients[\"dA\" + str(num_layers - 1)] = backwards_activation_output_0[\"dA\"]\n",
    "    \n",
    "    for i in reversed(range(num_layers - 1)):\n",
    "        past_layer = cached_values[i]\n",
    "        cur_dLayer = backwards_activation(gradients[\"dA\" + str(i + 1)], past_layer, \"relu\")\n",
    "        gradients[\"dW\" + str(i + 1)] = cur_dLayer[\"dW\"]\n",
    "        gradients[\"dB\" + str(i + 1)] = cur_dLayer[\"dB\"]\n",
    "        gradients[\"dA\" + str(i)] = cur_dLayer[\"dA\"]\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA0 (1, 2)\n",
      "dW1 (3, 4)\n",
      "dB1 (3, 1)\n",
      "dW2 (1, 3)\n",
      "dB2 (1, 1)\n",
      "dA1 (3, 2)\n",
      "{'dW2': array([[-0.39202432, -0.13325855, -0.04601089]]), 'dB2': array([[ 0.15187861]]), 'dA1': array([[-0.14175655,  0.48317296],\n",
      "       [ 0.01663708, -0.05670698],\n",
      "       [ 0.20472941, -0.69781407]]), 'dW1': array([[-0.45019464, -0.08570497, -0.15147489, -0.11528942],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.65018572,  0.1237779 ,  0.21876495,  0.16650473]]), 'dB1': array([[ 0.24158648],\n",
      "       [ 0.        ],\n",
      "       [-0.34890704]]), 'dA0': array([[ 0.        ,  1.43247838]])}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "a = np.random.randn(1, 2)\n",
    "a1 = np.random.randn(4, 2)\n",
    "b1 = np.random.randn(3, 4)\n",
    "w1 = np.random.randn(3, 1)\n",
    "z1 = np.random.randn(3, 2)\n",
    "cache1 = [{\"in_a\" : a1,\"b\" : b1,\"w\" : w1, \"z\" : z1}]\n",
    "a2 = np.random.randn(3, 2)\n",
    "b2 = np.random.randn(1, 1)\n",
    "w2 = np.random.randn(1, 3)\n",
    "z2 = np.random.randn(1, 2)\n",
    "cache2 = [{\"in_a\" : a2,\"b\" : b2,\"w\" : w2, \"z\" : z2}]\n",
    "megaCache = cache1 +  cache2\n",
    "y = np.array([[1,0]])\n",
    "# print(\"y\",y)\n",
    "# print(megaCache[0][\"in_a\"],\"in_a1\")\n",
    "# print(megaCache[0][\"b\"],\"b1\")\n",
    "# print(megaCache[0][\"w\"],\"w1\")\n",
    "# print(megaCache[0][\"z\"],\"z1\")\n",
    "# print(megaCache[1][\"in_a\"],\"in_a2\")\n",
    "# print(megaCache[1][\"b\"],\"b2\")\n",
    "# print(megaCache[1][\"w\"],\"w2\")\n",
    "# print(megaCache[1][\"z\"],\"z2\")\n",
    "\n",
    "\n",
    "grads = backprop(a, megaCache,y)\n",
    "print(\"dA0\", grads[\"dA0\"].shape)\n",
    "print(\"dW1\", grads[\"dW1\"].shape)\n",
    "print(\"dB1\", grads[\"dB1\"].shape)\n",
    "print(\"dW2\", grads[\"dW2\"].shape)\n",
    "print(\"dB2\", grads[\"dB2\"].shape)\n",
    "print(\"dA1\", grads[\"dA1\"].shape)\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizer(params,examples,learing_rate,epoch):\n",
    "    for i in range(epoch):\n",
    "        for X,y in examples:\n",
    "            a, cache = fowardPropagate(X,params)\n",
    "            grads = backprop(a,params,cache,Y)\n",
    "            for key,value in params:\n",
    "                if key[1] != \"A\": \n",
    "                    cur_grad = grads[\"d\" + key]\n",
    "                    value = value - (learning_rate * grad)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
