{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Vanilla Neural Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Image classifer with logistic regression\n",
    "#images shape(width,height,3(rgb))\n",
    "\n",
    "import numpy as np\n",
    "import scipy \n",
    "import os\n",
    "import matplotlib.pyplot as plot\n",
    "import math\n",
    "from scipy import ndimage\n",
    "\n",
    "#from basicFunctions import crossEntropyLoss \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossEntropyLoss(a,Y):\n",
    "    num_of_examples = Y.shape[1] #len(Y) doesn't work, need 2nd dimesnion\n",
    "    a = a.T\n",
    "    loss = - (1 / num_of_examples) * np.sum(np.dot((1 - Y), np.log(1 - a)) + np.dot(Y, np.log(a)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.414931599615\n"
     ]
    }
   ],
   "source": [
    "# Testing cross entropy function\n",
    "test_y = np.asarray([[1,1,1]])\n",
    "test_x = np.asarray([[.8,.9,.4]])\n",
    "print(crossEntropyLoss(test_x,test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output = [1231. 12312. 1231. ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    output = 1.0/(1.0+np.exp(-x))\n",
    "    cached_value = x\n",
    "    cached_value = {\"out_a\":x}\n",
    "    return output, cached_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x,leaky=0):\n",
    "    output = np.maximum(leaky,x)\n",
    "#     cached_value = x\n",
    "    cached_value = {\"out_a\":x}\n",
    "    return output, cached_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To generate numbers randomly with numpy, we will use np.random.randn(dimension_x, dimension_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params_shallow(input_layer, hidden_layer, output_layer):\n",
    "    w1 = np.random.randn(hidden_layer,input_layer)\n",
    "    w2 = np.random.randn(output_layer,hidden_layer)\n",
    "    #np.random.randn gives numbers larger than 1 so: (vanishing/exploading gradients)\n",
    "    w1,w2 = w1 * .01, w2 * .01\n",
    "    \n",
    "    #is it by one because of broadcasting \n",
    "    b1 = np.zeros((hidden_layer,1))\n",
    "    b2 = np.zeros((output_layer,1))\n",
    "    \n",
    "    assert(w1.shape == (hidden_layer, input_layer))\n",
    "    assert(w2.shape == (output_layer, hidden_layer))\n",
    "    assert(b1.shape == (hidden_layer,1))\n",
    "    assert(b2.shape == (output_layer,1))\n",
    "    \n",
    "    return {\"w1\":w1,\"w2\":w2,\"b1\":b1,\"b2\":b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1: [[ 0.00760019  0.0059487  -0.00544492]\n",
      " [ 0.00690711 -0.0013976  -0.00134813]\n",
      " [-0.01277502  0.01844119  0.01321794]\n",
      " [ 0.00948542 -0.00267607 -0.02244876]]\n",
      "w2: [[ 0.00272106  0.00050856 -0.0196768   0.00760006]\n",
      " [ 0.01116876 -0.00310177  0.00263714 -0.00661288]]\n",
      "b1: [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "b2: [[ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "params_shallow = initialize_params_shallow(3, 4, 2)\n",
    "print(\"w1:\", params_shallow[\"w1\"])\n",
    "print(\"w2:\", params_shallow[\"w2\"])\n",
    "print(\"b1:\", params_shallow[\"b1\"])\n",
    "print(\"b2:\", params_shallow[\"b2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params_deep(input_layer, hidden_layers, output_layer):\n",
    "#     np.random.seed(3) #non random random numbers (will get the same random numbers each time) used for testing\n",
    "    layers = {} #dictionary of weights and biasis\n",
    "    previous_layer = input_layer #sets the begining shape as shape of input layer\n",
    "    for i in range(len(hidden_layers)):\n",
    "        cur_layer = hidden_layers[i]\n",
    "        weights = np.random.randn(cur_layer,previous_layer)\n",
    "        weights *= .01 #because np.random.rand returns numbers too large, more efficeint durring gradient descensed \n",
    "        bias = np.zeros((cur_layer,1))\n",
    "        cur_weights_key = \"w\" + str(i + 1) # i + 1 because humans like things indexed at 0\n",
    "        cur_bias_key = \"b\" + str(i + 1)\n",
    "        layers[cur_weights_key] = weights\n",
    "        layers[cur_bias_key] = bias \n",
    "        \n",
    "        assert(layers[\"w\" + str(i + 1)].shape == (cur_layer, previous_layer))\n",
    "        assert(layers[\"b\" + str(i + 1)].shape == (cur_layer,1))\n",
    "        \n",
    "        previous_layer = cur_layer\n",
    "\n",
    "    layers[\"w\" + str(len(hidden_layers) + 1)] = np.random.randn(output_layer,previous_layer) * .01\n",
    "    layers[\"b\" + str(len(hidden_layers) + 1)] = np.zeros((output_layer,1))\n",
    "\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params_deep2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ed6ea026f7d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparams_deep1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_params_deep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams_deep2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_deep2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'params_deep2' is not defined"
     ]
    }
   ],
   "source": [
    "params_deep1 = initialize_params_deep(5,[4],3)\n",
    "for param in params_deep2:\n",
    "    print(param, params_deep2[param].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_deep2 = initialize_params_deep(5,[6,7,10],3)\n",
    "for param in params_deep2:\n",
    "    print(param, params_deep2[param].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward_prop(a,w,b): #a because a is from the past layer\n",
    "    w = w.T\n",
    "    a = a.T\n",
    "    z = np.dot(a,w) + b\n",
    "#     print(\"z\",z.shape,\"a\",a.shape[0],\"w\",w.shape[1],\"b\",b.shape)\n",
    "    assert(z.shape == (a.shape[0], w.shape[1] ))\n",
    "    #     assert(z.shape == np.dot(a,w).shape)\n",
    "    cached_inputs = {\"in_a\":a,\"w\":w,\"b\":b} #(a,w,b)\n",
    "    return z,cached_inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.randn(3,2)\n",
    "w = np.random.randn(1,3)\n",
    "b = np.random.randn(1) #(1,1) broadcasting\n",
    "\n",
    "X = np.array(X)\n",
    "w = np.array(w)\n",
    "b = np.array(b)\n",
    "z,_ = linear_forward_prop(X,w,b)\n",
    "print(\"X\",X.shape,\"w\",w.shape,\"b\",b.shape,\"z\",z.shape)\n",
    "print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singleFowardPropagation(X,w,b,activationType,leaky=0): #TODO change x to a\n",
    "#     cached_inputs = {\"w\":None,\"x\":None,\"b\":None,\"z\":None}\n",
    "    z,cached_values = linear_forward_prop(X,w,b)\n",
    "    if activationType == \"sigmoid\":\n",
    "        a, activation_cache = sigmoid(z)\n",
    "    if activationType == \"relu\":\n",
    "        a, activation_cache = relu(z,leaky=leaky)\n",
    "#     final_cache = c\n",
    "    cached_values.update(activation_cache)\n",
    "    return a, cached_values  #add asserts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "X = np.random.randn(3,2)\n",
    "w = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1) #(1,1) broadcasting\n",
    "a,cached_values = singleFowardPropagation(X,w,b,\"relu\")\n",
    "print(\"A\",a)\n",
    "print(\"cached_values\",cached_values)\n",
    "# print(X,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fowardPropagate(X,params):\n",
    "    num_layers = len(params)/2\n",
    "    a = X\n",
    "    cached_values = []\n",
    "    for i in range(1,num_layers):\n",
    "        w = params[\"w\" + str(i)]\n",
    "        b = params[\"b\" + str(i)]\n",
    "        past_a = a\n",
    "        a, cur_cached = singleFowardProgagation(past_a,w,b,\"relu\")\n",
    "        cached_values += [cur_cached] #TODO: get foramt of cached values \n",
    "    w = params[\"w\" + len(num_layers)]\n",
    "    b = params[\"b\" + len(num_layers)]\n",
    "    a, cur_cached = singleFowardProgagation(a,w,b,\"sigmoid\")\n",
    "    cached_values += [cur_cached] \n",
    "    #write assert statemt here\n",
    "    #get shape of a at the end \n",
    "    \n",
    "    return a, cached_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "X = np.random.randn(10,10,3)\n",
    "params = initialize_params_deep(5,[4,2,5],3)\n",
    "print(params)\n",
    "fowardPropagate(X, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
