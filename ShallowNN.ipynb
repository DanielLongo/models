{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Vanilla Neural Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Image classifer with logistic regression\n",
    "#images shape(width,height,3(rgb))\n",
    "\n",
    "###to knit to html ipython nbconvert --to html [NEWFILENAME].html\n",
    "import numpy as np\n",
    "import scipy \n",
    "import os\n",
    "import matplotlib.pyplot as plot\n",
    "import math\n",
    "from scipy import ndimage\n",
    "\n",
    "#from basicFunctions import crossEntropyLoss \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossEntropyLoss(a,Y):\n",
    "    num_of_examples = Y.shape[1] #len(Y) doesn't work, need 2nd dimesnion\n",
    "    a = a.T\n",
    "    loss = - (1.0 / num_of_examples) * (np.dot((1 - Y), np.log(1 - a)) + np.dot(Y, np.log(a)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.4149316]]\n"
     ]
    }
   ],
   "source": [
    "# Testing cross entropy function\n",
    "test_y = np.asarray([[1,1,1]])\n",
    "test_x = np.asarray([[.8,.9,.4]])\n",
    "print(crossEntropyLoss(test_x,test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output = [1231. 12312. 1231. ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    output = 1.0/(1.0+np.exp(-x))\n",
    "    cached_value = x\n",
    "    cached_value = {\"out_a\":x}\n",
    "    return output, cached_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x,leaky=0):\n",
    "    output = np.maximum(leaky,x)\n",
    "#     cached_value = x\n",
    "    cached_value = {\"out_a\":x}\n",
    "    return output, cached_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To generate numbers randomly with numpy, we will use np.random.randn(dimension_x, dimension_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params_shallow(input_layer, hidden_layer, output_layer):\n",
    "    w1 = np.random.randn(hidden_layer,input_layer)\n",
    "    w2 = np.random.randn(output_layer,hidden_layer)\n",
    "    #np.random.randn gives numbers larger than 1 so: (vanishing/exploading gradients)\n",
    "    w1,w2 = w1 * .01, w2 * .01\n",
    "    \n",
    "    #is it by one because of broadcasting \n",
    "    b1 = np.zeros((hidden_layer,1))\n",
    "    b2 = np.zeros((output_layer,1))\n",
    "    \n",
    "    assert(w1.shape == (hidden_layer, input_layer))\n",
    "    assert(w2.shape == (output_layer, hidden_layer))\n",
    "    assert(b1.shape == (hidden_layer,1))\n",
    "    assert(b2.shape == (output_layer,1))\n",
    "    \n",
    "    return {\"w1\":w1,\"w2\":w2,\"b1\":b1,\"b2\":b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1: [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]\n",
      " [ 0.01744812 -0.00761207  0.00319039]\n",
      " [-0.0024937   0.01462108 -0.02060141]]\n",
      "w2: [[-0.00322417 -0.00384054  0.01133769 -0.01099891]\n",
      " [-0.00172428 -0.00877858  0.00042214  0.00582815]]\n",
      "b1: [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "b2: [[ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "params_shallow = initialize_params_shallow(3, 4, 2)\n",
    "print(\"w1:\", params_shallow[\"w1\"])\n",
    "print(\"w2:\", params_shallow[\"w2\"])\n",
    "print(\"b1:\", params_shallow[\"b1\"])\n",
    "print(\"b2:\", params_shallow[\"b2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test case for initialize_params_shallow using seed(1)\n",
    "##### (3,4,2)\n",
    "<table style:\"width-80%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "         <b> w1:</b> [[ 0.01624345 -0.00611756 -0.00528172]\n",
    "         [-0.01072969  0.00865408 -0.02301539]\n",
    "         [ 0.01744812 -0.00761207  0.00319039]\n",
    "         [-0.0024937   0.01462108 -0.02060141]]\n",
    "        </td>\n",
    "        <td>\n",
    "        w2: [[-0.00322417 -0.00384054  0.01133769 -0.01099891]\n",
    " [-0.00172428 -0.00877858  0.00042214  0.00582815]]\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "        b1: [[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]\n",
    "        </td>\n",
    "        <td>\n",
    "b2: [[ 0.]\n",
    " [ 0.]]\n",
    " </td>\n",
    "     </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_params_deep(input_layer, hidden_layers, output_layer):\n",
    "#     np.random.seed(3) #non random random numbers (will get the same random numbers each time) used for testing\n",
    "    layers = {} #dictionary of weights and biasis\n",
    "    previous_layer = input_layer #sets the begining shape as shape of input layer\n",
    "    for i in range(len(hidden_layers)):\n",
    "        cur_layer = hidden_layers[i]\n",
    "        weights = np.random.randn(cur_layer,previous_layer)\n",
    "        weights *= .01 #because np.random.rand returns numbers too large, more efficeint durring gradient descensed \n",
    "        bias = np.zeros((cur_layer,1))\n",
    "        cur_weights_key = \"w\" + str(i + 1) # i + 1 because humans like things indexed at 0\n",
    "        cur_bias_key = \"b\" + str(i + 1)\n",
    "        layers[cur_weights_key] = weights\n",
    "        layers[cur_bias_key] = bias \n",
    "        \n",
    "        assert(layers[\"w\" + str(i + 1)].shape == (cur_layer, previous_layer))\n",
    "        assert(layers[\"b\" + str(i + 1)].shape == (cur_layer,1))\n",
    "        \n",
    "        previous_layer = cur_layer\n",
    "\n",
    "    layers[\"w\" + str(len(hidden_layers) + 1)] = np.random.randn(output_layer,previous_layer) * .01\n",
    "    layers[\"b\" + str(len(hidden_layers) + 1)] = np.zeros((output_layer,1))\n",
    "\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 (4, 5)\n",
      "b1 (4, 1)\n",
      "w2 (3, 4)\n",
      "b2 (3, 1)\n"
     ]
    }
   ],
   "source": [
    "params_deep1 = initialize_params_deep(5,[4],3)\n",
    "for param in params_deep2:\n",
    "    print(param, params_deep1[param].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 (4, 5)\n",
      "[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 (4, 1)\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "w2 (3, 4)\n",
      "[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 (3, 1)\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "params_deep2 = initialize_params_deep(5,[4],3)\n",
    "for param in params_deep2:\n",
    "    print(param, params_deep2[param].shape)\n",
    "    print(params_deep2[param])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testcase initialize_params_deep \n",
    "##### seed(3) (5,[3],3)\n",
    "<table style:\"width-80%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "w1 (4, 5)\n",
    "[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
    " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
    " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
    " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
    " </td>\n",
    " <td>\n",
    "b1 (4, 1)\n",
    "[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]\n",
    "</td>\n",
    "<tr>\n",
    "<td>\n",
    "w2 (3, 4)\n",
    "[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
    " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
    " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
    "</td>\n",
    "<td>\n",
    "b2 (3, 1)\n",
    "[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]\n",
    " </td>\n",
    " </tr>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward_prop(a,w,b): #a because a is from the past layer\n",
    "    print(\"Before Transpose -- A:\",a.shape,\"W:\",w.shape,\"B:\",b.shape)\n",
    "    w = w.T\n",
    "    a = a.T\n",
    "    print(\"After Transpose -- A:\",a.shape,\"W:\",w.shape,\"B:\",b.shape)\n",
    "    z = np.dot(a,w) + b\n",
    "    print(\"z:\",z.shape)\n",
    "#     print(\"z\",z.shape,\"a\",a.shape[0],\"w\",w.shape[1],\"b\",b.shape)\n",
    "    assert(z.shape == (a.shape[0], w.shape[1] ))\n",
    "    #     assert(z.shape == np.dot(a,w).shape)\n",
    "    cached_inputs = {\"in_a\":a,\"w\":w,\"b\":b} #(a,w,b)\n",
    "    return z,cached_inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (3, 2) w (1, 3) b (1,) z (2, 1)\n",
      "[[ 3.26295337]\n",
      " [-1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.randn(3,2)\n",
    "w = np.random.randn(1,3)\n",
    "b = np.random.randn(1) #(1,1) broadcasting\n",
    "\n",
    "X = np.array(X)\n",
    "w = np.array(w)\n",
    "b = np.array(b)\n",
    "z,_ = linear_forward_prop(X,w,b)\n",
    "print(\"X\",X.shape,\"w\",w.shape,\"b\",b.shape,\"z\",z.shape)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test case for linear_foward_prop\n",
    "##### seed(1) x=(3,2) w=(1,3) b=(1)\n",
    "<table style:\"width-80%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "X (3, 2) w (1, 3) b (1,) z (2, 1)\n",
    "</td>\n",
    "</tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "[[ 3.26295337]\n",
    " [-1.23429987]]\n",
    " </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def singleFowardPropagation(X,w,b,activationType,leaky=0): #TODO change x to a\n",
    "#     cached_inputs = {\"w\":None,\"x\":None,\"b\":None,\"z\":None}\n",
    "    z,cached_values = linear_forward_prop(X,w,b)\n",
    "    if activationType == \"sigmoid\":\n",
    "        a, activation_cache = sigmoid(z)\n",
    "    if activationType == \"relu\":\n",
    "        a, activation_cache = relu(z,leaky=leaky)\n",
    "#     final_cache = c\n",
    "    cached_values.update(activation_cache)\n",
    "    return a, cached_values  #add asserts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relu A [[ 3.43896131]\n",
      " [ 0.        ]]\n",
      "Sigmoud A [[ 0.96890023]\n",
      " [ 0.11013289]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "X = np.random.randn(3,2)\n",
    "w = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1) #(1,1) broadcasting\n",
    "a,cached_values = singleFowardPropagation(X,w,b,\"relu\")\n",
    "print(\"Relu A\",a)\n",
    "\n",
    "a,cached_values = singleFowardPropagation(X,w,b,\"sigmoid\")\n",
    "print(\"Sigmoud A\",a)\n",
    "# print(X,w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relu A [[ 3.43896131]\n",
    " [ 0.        ]]\n",
    "Sigmoud A [[ 0.96890023]\n",
    " [ 0.11013289]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fowardPropagate(X,params):\n",
    "    print(\"len of params\", len(params))\n",
    "    num_layers = len(params)/2\n",
    "    #assert len params is even\n",
    "    num_layers = int(num_layers)\n",
    "    a = X\n",
    "    cached_values = []\n",
    "    for i in range(1,num_layers):\n",
    "        w = params[\"w\" + str(i)]\n",
    "        b = params[\"b\" + str(i)]\n",
    "        past_a = a\n",
    "        a, cur_cached = singleFowardPropagation(past_a,w,b,\"relu\")\n",
    "        cached_values += [cur_cached] #TODO: get foramt of cached values \n",
    "    w = params[\"w\" + str(num_layers)]\n",
    "    b = params[\"b\" + str(num_layers)]\n",
    "    a, cur_cached = singleFowardPropagation(a,w,b,\"sigmoid\")\n",
    "    cached_values += [cur_cached] \n",
    "    #write assert statemt here\n",
    "    #get shape of a at the end \n",
    "    \n",
    "    return a, cached_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of params 6\n",
      "Before Transpose -- A: (5, 4) W: (4, 5) B: (4, 1)\n",
      "After Transpose -- A: (4, 5) W: (5, 4) B: (4, 1)\n",
      "z: (4, 4)\n",
      "Before Transpose -- A: (4, 4) W: (3, 4) B: (4, 1)\n",
      "After Transpose -- A: (4, 4) W: (4, 3) B: (4, 1)\n",
      "z: (4, 3)\n",
      "Before Transpose -- A: (4, 3) W: (1, 4) B: (1, 1)\n",
      "After Transpose -- A: (3, 4) W: (4, 1) B: (1, 1)\n",
      "z: (3, 1)\n",
      "a: [[ 0.18411956]\n",
      " [ 0.71696431]\n",
      " [ 0.99627442]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6)\n",
    "X = np.random.randn(5,4)\n",
    "w1 = np.random.randn(4,5)\n",
    "b1 = np.random.randn(4,1)\n",
    "w2 = np.random.randn(3,4)\n",
    "b2 = np.random.randn(4,1)\n",
    "w3 = np.random.randn(1,4)\n",
    "b3 = np.random.randn(1,1)\n",
    "params = {\n",
    "    \"w1\":w1,\n",
    "    \"b1\":b1,\n",
    "    \"w2\":w2,\n",
    "    \"b2\":b2,\n",
    "    \"w3\":w3,\n",
    "    \"b3\":b3\n",
    "}\n",
    "a, cached_values = fowardPropagate(X, params)\n",
    "print(\"a:\",a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearBackprop(cached_values, dZ):\n",
    "    a = cached_values[\"in_a\"]\n",
    "    w = cached_values[\"w\"]\n",
    "    b = cached_values[\"b\"]\n",
    "    number_examples = a.shape[1]\n",
    "    dW = (1/number_examples) * np.dot(dZ,a.T)\n",
    "    dB = (1/number_examples) * np.sum(dZ, axis = 1, keepdims= True)\n",
    "    dA = np.dot(w.T, dZ)\n",
    "#write asserts for dW dB & dA TODO\n",
    "    return {\"dW\":dW,\"dB\":dB,\"dA\":dA}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dA': array([[ 0.51822968, -0.19517421],\n",
       "        [-0.40506361,  0.15255393],\n",
       "        [ 2.37496825, -0.89445391]]),\n",
       " 'dB': array([[ 0.50629448]]),\n",
       " 'dW': array([[-0.10076895,  1.40685096,  1.64992505]])}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "dZ = np.random.randn(1,2)\n",
    "a = np.random.randn(3,2)\n",
    "w = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "cached_values = {\"w\": w, \"in_a\":a,\"b\":b}\n",
    "linearBackprop(cached_values, dZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'dA': array([[ 0.51822968, -0.19517421],\n",
    "        [-0.40506361,  0.15255393],\n",
    "        [ 2.37496825, -0.89445391]]),\n",
    " 'dB': array([[ 0.50629448]]),\n",
    " 'dW': array([[-0.10076895,  1.40685096,  1.64992505]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    z = cache\n",
    "    dZ = np.array(dA, copy=True) #convert dA to np array\n",
    "    dZ[z <= 0] = 0 \n",
    "    \n",
    "    return dZ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0  0]\n"
     ]
    }
   ],
   "source": [
    "dZ = relu_backward([-1,3,2], np.array([2,-1,0]))\n",
    "print(dZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    z = cache\n",
    "    s = 1/(1 + np.exp(-z))\n",
    "    dz = dA * s * (1-s) \n",
    "    \n",
    "    return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backwards_activation(dA, cached_values, activation_type):\n",
    "    if activation_type == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, cached_values[\"z\"])\n",
    "    if activation_type == \"relu\":\n",
    "        dZ = relu_backward(dA, cached_values[\"z\"])\n",
    "    gradients = linearBackprop(cached_values, dZ)\n",
    "    \n",
    "    return gradients\n",
    "    \n",
    "    #todo write optimizer function\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dA': array([[ 0.44090989,  0.        ],\n",
       "        [ 0.37883606,  0.        ],\n",
       "        [-0.2298228 ,  0.        ]]),\n",
       " 'dB': array([[-0.20837892]]),\n",
       " 'dW': array([[ 0.44513824,  0.37371418, -0.10478989]])}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "dA = np.random.randn(1,2)\n",
    "a = np.random.randn(3,2)\n",
    "w = np.random.randn(1,3)\n",
    "b = np.random.randn(1,1)\n",
    "z = np.random.randn(1,2)\n",
    "cached_values = {\"w\": w, \"in_a\":a,\"b\":b, \"z\":z}\n",
    "backwards_activation(dA, cached_values, activation_type=\"relu\")\n",
    "#working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
